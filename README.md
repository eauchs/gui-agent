
# Agent de Navigation GUI √† Deux Niveaux

Ce projet est un agent d'automatisation d'interface graphique (GUI) pour macOS, con√ßu avec une architecture √† deux niveaux pour s√©parer la perception visuelle de la prise de d√©cision strat√©gique. Il est capable de comprendre des objectifs utilisateur de haut niveau, d'analyser l'√©cran et d'ex√©cuter des s√©quences d'actions (clics, frappe de texte, raccourcis clavier) pour atteindre ces objectifs.

## ü§ñ Architecture

L'agent repose sur deux mod√®les de langage (LLM) qui collaborent :

1.  **Le VLM Frontend (Perception)** : Un mod√®le de langage multimodal (Vision Language Model) qui agit comme les "yeux" de l'agent. Il re√ßoit une capture d'√©cran et une instruction *sp√©cifique* du superviseur. Sa seule t√¢che est d'analyser l'image et de proposer une s√©quence de micro-actions (par exemple, "cliquer sur le bouton √† la position [x, y]", "taper 'hello world'") dans un format JSON strict.

      * **Mod√®le utilis√© (configurable)** : `internvl3-8b-instruct`

2.  **Le LLM Backend (Strat√©gie)** : Un mod√®le de langage standard qui agit comme le "cerveau" de l'agent. Il re√ßoit l'objectif global de l'utilisateur, analyse la sortie (ou l'√©chec) du VLM, √©value si le plan est pertinent et prend la d√©cision finale.

      * **Donner une nouvelle instruction au VLM** pour affiner l'action.
      * **Approuver la s√©quence d'actions** propos√©e par le VLM pour ex√©cution.
      * **Corriger ou proposer sa propre s√©quence d'actions** si le VLM est bloqu√© ou fait des erreurs r√©p√©t√©es.
      * **D√©terminer si la t√¢che est termin√©e** ou a √©chou√©.
      * **Mod√®le utilis√© (configurable)** : `qwen/qwen3-8b`

Cette s√©paration permet de confier la t√¢che complexe d'analyse visuelle √† un mod√®le sp√©cialis√©, tout en utilisant un LLM plus "g√©n√©raliste" et strat√©gique pour la logique, la correction d'erreurs et la planification √† long terme.

## ‚ú® Fonctionnalit√©s

  * **Contr√¥le de l'interface graphique** : Automatise les clics, doubles-clics, la saisie de texte, le d√©filement et les raccourcis clavier.
  * **Feedback visuel** : Affiche des superpositions (overlays) √† l'√©cran pour indiquer quelle action est en cours d'ex√©cution.
  * **Feedback audio** : Joue des sons pour notifier les diff√©rentes √©tapes (nouvelle t√¢che, succ√®s, erreur).
  * **Logging d√©taill√©** : Enregistre les captures d'√©cran, les d√©cisions des mod√®les et les actions ex√©cut√©es pour chaque √©tape, facilitant le d√©bogage.
  * **Configuration flexible** : Les mod√®les et les points d'acc√®s API sont configurables via des variables d'environnement.
  * **Gestion des erreurs robuste** : Le superviseur (Qwen) peut d√©tecter lorsque le VLM √©choue et tenter de corriger le tir ou de reformuler les instructions.

## üõ†Ô∏è Installation

### Pr√©requis

  * **Python 3.8+**
  * Un **serveur de mod√®les local** compatible avec l'API OpenAI (par exemple, [LM Studio](https://lmstudio.ai/), [Ollama](https://ollama.com/)). Vous devrez y charger les mod√®les VLM et LLM requis.
  * **macOS** (car `pyautogui` et `pynput` ont des comportements qui peuvent varier selon l'OS).

### √âtapes

1.  **Clonez le d√©p√¥t :**

    ```bash
    git clone https://votre-url-de-depot.git
    cd nom-du-repertoire
    ```

2.  **Cr√©ez un environnement virtuel et activez-le :**

    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Installez les d√©pendances Python :**

    ```bash
    pip install -r requirements.txt
    ```

    *Note : Si vous n'avez pas de fichier `requirements.txt`, installez les paquets manuellement :*

    ```bash
    pip install openai pyautogui Pillow pynput rich sounddevice soundfile numpy
    ```

## ‚öôÔ∏è Configuration

L'agent est configur√© √† l'aide de variables d'environnement. Vous pouvez les d√©finir dans votre terminal avant de lancer le script ou utiliser un fichier `.env`.

1.  **Point d'acc√®s API** : Assurez-vous que votre serveur local est en cours d'ex√©cution. L'URL par d√©faut est `http://localhost:1234/v1`.

    ```bash
    export OPENAI_API_BASE_URL="http://localhost:1234/v1"
    ```

2.  **Nom des mod√®les** : Les noms doivent correspondre **exactement** √† ceux charg√©s dans votre serveur local.

    ```bash
    # Mod√®le pour l'analyse visuelle (VLM)
    export VLM_MODEL_NAME_FOR_API="internvl3-8b-instruct"

    # Mod√®le pour la strat√©gie (LLM)
    export QWEN_MODEL_NAME_FOR_API="qwen/qwen3-8b"
    ```

## ‚ñ∂Ô∏è Lancement

Une fois les d√©pendances install√©es et les variables d'environnement configur√©es, lancez le script principal depuis votre terminal :

```bash
python nom_du_script.py
```

L'agent vous demandera de saisir un objectif global.

### Exemples d'objectifs

  * "Ouvre Chrome, va sur https://www.google.com/search?q=google.com et cherche des images de chats mignons."
  * "Ouvre le terminal, liste les fichiers dans le r√©pertoire actuel, puis cr√©e un nouveau dossier appel√© 'test\_agent'."
  * "V√©rifie s'il y a des mises √† jour syst√®me disponibles dans les Pr√©f√©rences Syst√®me."

Pour arr√™ter l'agent, vous pouvez taper `exit` ou `quit` lorsque vous √™tes invit√© √† saisir un objectif, ou utiliser `Ctrl+C` dans le terminal.

## üìù Fichiers et Dossiers g√©n√©r√©s

Pendant son ex√©cution, l'agent cr√©e automatiquement :

  * `agent_gui_screenshots_api/` : Un dossier contenant une capture d'√©cran pour chaque √©tape de la t√¢che.
  * `agent_gui_screenshots_api/detailed_interaction_log.txt` : Un fichier journal tr√®s d√©taill√©, enregistrant les prompts, les r√©ponses brutes des mod√®les et les actions ex√©cut√©es. Utile pour le d√©bogage.
  * `audio_feedback/` : Contient les fichiers sonores g√©n√©r√©s pour le feedback audio.
